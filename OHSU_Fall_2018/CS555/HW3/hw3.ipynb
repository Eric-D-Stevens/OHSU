{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you what use ngrams.py you should use python2\n",
    "# Or, otherwise, you need to modify ngrams.py by yourself in order to use it in python3.\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "from string import punctuation\n",
    "import re\n",
    "import numpy as np\n",
    "from ngrams import ngrams\n",
    "from collections import defaultdict\n",
    "from bitweight import BitWeight, BitWeightRangeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "small_corpus = ['Why dont we start here',\n",
    "                  'Why dont we end there',\n",
    "                  'Let us start with a few other examples',\n",
    "                  'We never start with an example with so few tokens',\n",
    "                  'Tokens can be words that we start with in example docs']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokens = [sentence.split(' ') for sentence in corpus]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3: Language Modeling\n",
    "For this part of the assignment, you will implement two simple count-based n-gram language models: one based on maximum-likelihood estimation, and another based on Witten-Bell smoothing. The data you will be using is a subset of the Penn Treebank's tagged Wall Street Journal articles on which we have done some initial processing. There are two versions of the data for this assignment:\n",
    "\n",
    "##### wsj.pos.gz\n",
    "##### wsj-normalized.pos.gz\n",
    "The difference is that, in the second (normalized) version of the data, we have collapsed some entries from certain tag categories (e.g. CDs, NNPs, etc.) into type-tokens to help reduce sparsity. Take a look at the data and see for yourself. Consider: what would be the benefits and drawbacks to this method of sparsity reduction? Note that, for this part of the assignment, the tags are un-necessary, so you'll want to work with the un-normalized version of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: produce a tag-free corpus\n",
    "\n",
    "For this task, you have two jobs. \n",
    "* First, you need to write a function to filter out all tags. \n",
    "* Second, Make sure your code works for both wsj.pos.gz and wsj-normalized.pos.gz\n",
    "\n",
    "####What to turn in\n",
    "* your code\n",
    "* some samples to show me that your code works as it should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_list(filename):\n",
    "    with open(filename, 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        no_tags = re.sub('(/[A-Z$]{2,4})|(\\.\\s+[a-z])|(/[.,$])|[\\\\,/\\'`]', '', content)\n",
    "        return re.split('[\\n]',no_tags)\n",
    "        \n",
    "s = file_to_list(\"./wsj.pos\")\n",
    "#for x in range(10): print(s[x],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood\n",
    "Now, start by producing code to compute maximum-likelihood estimate probabilities. Your code should be configurable with respect to the n-gram order- i.e., you should be able to set it to compute bigram, trigram, 4-gram, etc. probabilities. Refer to J&M and the lecture slides for definitions as needed. If you would like to write your own n-gram tokenization code, feel free to do so, but you may also use the ngrams.py utility class which contains a routine to take a list of tokens and produce a stream of n-grams with appropriate padding for the start and end of sentences.\n",
    "\n",
    "#### Tip: \n",
    "* Start with a very small \"toy\" corpus of just a couple of sentences for debugging. \n",
    "\n",
    "* As discussed in class, I strongly recommend using nested defaultdicts as the foundational data structure for your language model, where the \"outer\" key is the prefix, and the value retrieved by that prefix is a second defaultdict  containing possible suffices for that prefix, each of which is an \"inner\" key. E.g., p(\"TRUTHS\" | \"HOLD THESE\") would be retrieved by first looking up \"HOLD THESE\" and then from the resulting dictionary, looking up \"TRUTHS\": prob = trigrams[(\"HOLD\",\"THESE\")][\"TRUTHS\"] . Note that this arrangement makes it very easy to e.g. find out the number of times a given history occurs, the total probability mass assigned to all of a history's continuations, etc., all of which will be extremely helpful in the next part of the assignment.\n",
    "\n",
    "* Use tuples to represent prefixes. E.g., instead of the string \"HOLD THESE\", use the tuple (\"HOLD\", \"THESE\"). Note that, in Python, lists are mutable, and therefore may not be used as keys in dictionaries- but tuples are immutable, and so make excellent keys.\n",
    "\n",
    "* Don't forget about numerical underflow issues! You'll want to represent probabilities as negative base-2 log probabilities, and modify your arithmetic accordingly. I recommend experimenting with [the bitweight Python library](https://github.com/stevenbedrick/bitweight) (see its unit tests for example usage).\n",
    "* \n",
    "\n",
    "#### What to turn in:\n",
    "* your code \n",
    "* use your code to create a simple language model for small_corpus named as small_lm and show me that your output is correct(This is a small coupus so you could manully calculate the probalility).\n",
    "* use your code to create language model for wsj.pos.gz named as wsj_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'examples',)  :  </S_0>  :  1.0\n",
      "(u'few',)  :  tokens  :  0.5\n",
      "(u'few',)  :  other  :  0.5\n",
      "(u'in',)  :  example  :  1.0\n",
      "(u'We',)  :  never  :  1.0\n",
      "(u'Why',)  :  dont  :  1.0\n",
      "(u'end',)  :  there  :  1.0\n",
      "(u'start',)  :  with  :  0.75\n",
      "(u'start',)  :  here  :  0.25\n",
      "(u'other',)  :  examples  :  1.0\n",
      "(u'here',)  :  </S_0>  :  1.0\n",
      "(u'words',)  :  that  :  1.0\n",
      "(u'an',)  :  example  :  1.0\n",
      "(u'we',)  :  start  :  0.666666666667\n",
      "(u'we',)  :  end  :  0.333333333333\n",
      "(u'dont',)  :  we  :  1.0\n",
      "(u'there',)  :  </S_0>  :  1.0\n",
      "('<S_0>',)  :  Tokens  :  0.2\n",
      "('<S_0>',)  :  We  :  0.2\n",
      "('<S_0>',)  :  Let  :  0.2\n",
      "('<S_0>',)  :  Why  :  0.4\n",
      "(u'so',)  :  few  :  1.0\n",
      "(u'us',)  :  start  :  1.0\n",
      "(u'a',)  :  few  :  1.0\n",
      "(u'example',)  :  docs  :  0.5\n",
      "(u'example',)  :  with  :  0.5\n",
      "(u'docs',)  :  </S_0>  :  1.0\n",
      "(u'Tokens',)  :  can  :  1.0\n",
      "(u'never',)  :  start  :  1.0\n",
      "(u'Let',)  :  us  :  1.0\n",
      "(u'can',)  :  be  :  1.0\n",
      "(u'be',)  :  words  :  1.0\n",
      "(u'with',)  :  a  :  0.25\n",
      "(u'with',)  :  an  :  0.25\n",
      "(u'with',)  :  so  :  0.25\n",
      "(u'with',)  :  in  :  0.25\n",
      "(u'tokens',)  :  </S_0>  :  1.0\n",
      "(u'that',)  :  we  :  1.0\n",
      "Total Probability:  28.0\n",
      "Count:  38\n",
      "(u'examples',)\n",
      "(u'few',)\n",
      "(u'in',)\n",
      "(u'We',)\n",
      "(u'Why',)\n",
      "(u'end',)\n",
      "(u'start',)\n",
      "(u'other',)\n",
      "(u'here',)\n",
      "(u'words',)\n",
      "(u'an',)\n",
      "(u'we',)\n",
      "(u'dont',)\n",
      "(u'there',)\n",
      "('<S_0>',)\n",
      "(u'so',)\n",
      "(u'us',)\n",
      "(u'a',)\n",
      "(u'example',)\n",
      "(u'docs',)\n",
      "(u'Tokens',)\n",
      "(u'never',)\n",
      "(u'Let',)\n",
      "(u'can',)\n",
      "(u'be',)\n",
      "(u'with',)\n",
      "(u'tokens',)\n",
      "(u'that',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3999999999999999"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# generates a model with just counts\n",
    "def count_builder(corpus, order):\n",
    "    \n",
    "    #ngram\n",
    "    ng = ngrams(corpus, order)\n",
    "\n",
    "    # describe model datatype\n",
    "    model = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    # loop to build embedded defaultdict    \n",
    "    for gram in ng: \n",
    "        if not gram[1] in model[gram[0]]:\n",
    "            model[gram[0]][gram[1]] = 1\n",
    "        else:\n",
    "            model[gram[0]][gram[1]] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Takes an input of a count model and outputs a MLE model\n",
    "def max_likelihood(count_model):\n",
    "    \n",
    "    #returned MLE Model with BitWeight Probabilities\n",
    "    model = defaultdict(lambda: defaultdict(lambda: BitWeight(0)))\n",
    "    \n",
    "    # for prefixes in model\n",
    "    for prefix, suffix_dict in count_model.iteritems():\n",
    "        w_minus = BitWeight(0)\n",
    "        # for words with hist prefix\n",
    "        for suffix, count in suffix_dict.iteritems():\n",
    "            w_minus += BitWeight(count) # count number of tokens\n",
    "        for suffix, count in suffix_dict.iteritems():\n",
    "            model[prefix][suffix] = BitWeight.__itruediv__(BitWeight(count),w_minus) # set output probabilities\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# model printer\n",
    "def model_printer(model):\n",
    "    total = BitWeight(0)\n",
    "    count = 0 \n",
    "    for prefix, suffix_dict in model.iteritems() :\n",
    "        for suffix, value in suffix_dict.iteritems():\n",
    "            print(prefix,\" : \", suffix, \" : \", value.real())\n",
    "            count += 1\n",
    "            total += value\n",
    "    print(\"Total Probability: \",total.real())\n",
    "    print(\"Count: \",count)\n",
    "            \n",
    "\n",
    "small_tokens = tokenize(small_corpus) # turn small corpus into ngrams\n",
    "count_model = count_builder(small_tokens, 2) # build bigram count model\n",
    "ml_model = max_likelihood(count_model) # build max likelihood model from count model\n",
    "model_printer(ml_model) # print entire model\n",
    "\n",
    "x=ngrams(small_tokens,2)\n",
    "for i in ml_model:\n",
    "    print(i)\n",
    "ml_model[('<S_0>',)]['Why'].real()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing\n",
    "\n",
    "Once you’ve got an unsmoothed model working, move on to implementing Witten-Bell smoothing. Refer to the slides and J&M for details on how that ought to work.\n",
    "\n",
    "#### Tip: \n",
    "* You can modify an already-populated defaultdict to change its default value (for example, to store a default backoff value for a particular history) by changing the object’s default_factory attribute. Consult the documentation for examples of how this works.\n",
    "* As defined, W-B smoothing is highly recursive; you may find it more efficient to re-frame the algorithm in iterative terms.\n",
    "* As in the previous section, start small.\n",
    "* [This may offer you some help on how to implement Witten-Bell smoothing](http://www.ee.columbia.edu/~stanchen/e6884/labs/lab3/x207.html)\n",
    "\n",
    "\n",
    "#### What to turn in:\n",
    "* your code \n",
    "* use your code to create a simple smoothed language model based on small_lm  and show me that your output is correct(This is a small coupus so you could manully calculate the probalility).\n",
    "* use your code to create a smoothed language model based on wsj_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation via Perplexity\n",
    "Explore the effects of n-gram order using perplexity. Perform ten-fold cross-validation on the WSJ corpus. On each iteration, this will give you a different 90/10 training/test split; train a smoothed language model on the 9 training sections, and compute the average per-token perplexity of the tenth section. The slides from the language modeling lecture give the equation for perplexity computation (as does J&M chapter 4); you'll need to modify the equation a bit, since we're using log-probabilities. \n",
    "\n",
    "Now, try this for unigram, bigram, trigram, and 4-gram models. \n",
    "\n",
    "#### What to turn in\n",
    "* your cross-validation function. You are not suppose to use any cross-validation function from any module. You should implement it by yourself.\n",
    "* your perplexity function\n",
    "* cross-validation result for unigram, bigram, trigram, and 4-gram models on wsj.pos.gz\n",
    "* cross-validation result for unigram, bigram, trigram, and 4-gram models on wsj-normalized.pos.gz.\n",
    "* Answer following 2 questions: \n",
    "    * How does perplexity change as the model order size increases?\n",
    "    * How does perplexity change as the data changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wb_model_builder(tokens, order):\n",
    "    \n",
    "    # build count model and MLE model to calculate WB values\n",
    "    count_mod = count_builder(tokens, order)\n",
    "    mle_mod = max_likelihood(count_mod)\n",
    "    \n",
    "    \n",
    "    # Base Case: Unigram Model Builder\n",
    "    if order == 1:\n",
    "        \n",
    "        # get tokens and types to calc lmbda(.)\n",
    "        tks = sum(count_mod[()].values())\n",
    "        tps = len(count_mod[()].keys())\n",
    "        \n",
    "        # use above to build lambda(.)\n",
    "        lambda_dot = BitWeight(tks,(tks+tps))\n",
    "        one_minus_lambda_dot = BitWeight(tps,(tks+tps))\n",
    "        one_over_V = BitWeight(1,tps)\n",
    "        \n",
    "        # create return for unseen values Pwb(.) = (1 - lambda) * (1/V)\n",
    "        default_return = one_minus_lambda_dot*one_over_V\n",
    "        \n",
    "        # declare WB unigram model to be handed back\n",
    "        wb_model = defaultdict(lambda: defaultdict(lambda: default_return))\n",
    "        \n",
    "        # build WB unigram model\n",
    "        for prefix, suffix_dict in mle_mod.iteritems() :\n",
    "            for suffix, mle_prob in suffix_dict.iteritems():\n",
    "                # Set WB Probability: Pwb(w) = lambda(.)*Pmle + one_minus_lambda(.)*one_over_V\n",
    "                wb_model[prefix][suffix] = (lambda_dot * mle_prob) + (one_minus_lambda_dot*one_over_V)\n",
    "        \n",
    "        # return WB unigram model\n",
    "        return wb_model\n",
    "    \n",
    "    \n",
    "    # Recursive Case: If the order is greater than 1\n",
    "    \n",
    "    # get the model of P(w|h_n-1)\n",
    "    wb_minus_one_model = wb_model_builder(tokens, order-1)\n",
    "    \n",
    "    # retrieve default value from lower order model\n",
    "    default_return = wb_minus_one_model[\"x\"][\"x\"] # xx will not be in any model\n",
    "\n",
    "    # declare WB model with inhereted default\n",
    "    wb_model = defaultdict(lambda: defaultdict(lambda: default_return))\n",
    "    \n",
    "    # for each history... \n",
    "    for prefix, suffix_dict in count_mod.iteritems():\n",
    "\n",
    "        # calculate history parameters\n",
    "        c_hist = sum(suffix_dict.values())\n",
    "        N_one_plus = len(suffix_dict.keys())\n",
    "\n",
    "        # build lambda terms\n",
    "        lambda_hist = BitWeight(c_hist,(c_hist+N_one_plus))\n",
    "        one_minus_lambda_hist = BitWeight(N_one_plus,(c_hist+N_one_plus))\n",
    "        \n",
    "        # for each word that follows that history ... \n",
    "        for suffix, wb_minus_prob in mle_mod[prefix].iteritems():\n",
    "\n",
    "            # Set WB Probability: Pwb(w|hist) = lambda_hist*Pmle(w|hist) + one_minus_lambda_hist*Pwb(w|hist-1)\n",
    "            lambda_hist_x_mle = lambda_hist*mle_mod[prefix][suffix]\n",
    "            one_minus_lambda_x_wb_minus = one_minus_lambda_hist*wb_minus_one_model[prefix[1:]][suffix]\n",
    "            wb_model[prefix][suffix] = lambda_hist_x_mle + one_minus_lambda_x_wb_minus\n",
    "    \n",
    "    # return WB ngram model\n",
    "    return wb_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<S_2>', '<S_3>', '<S_4>', u'Tokens', u'can')  :  be  :  0.507575757576\n",
      "(u'we', u'start', u'with', u'in', u'example')  :  docs  :  0.954071969697\n",
      "('<S_0>', '<S_1>', '<S_2>', '<S_3>', '<S_4>')  :  Tokens  :  0.117845117845\n",
      "('<S_0>', '<S_1>', '<S_2>', '<S_3>', '<S_4>')  :  We  :  0.117845117845\n",
      "('<S_0>', '<S_1>', '<S_2>', '<S_3>', '<S_4>')  :  Let  :  0.117845117845\n",
      "('<S_0>', '<S_1>', '<S_2>', '<S_3>', '<S_4>')  :  Why  :  0.228956228956\n",
      "('<S_4>', u'We', u'never', u'start', u'with')  :  an  :  0.90459280303\n",
      "(u'with', u'a', u'few', u'other', u'examples')  :  </S_4>  :  0.507575757576\n",
      "(u'Why', u'dont', u'we', u'start', u'here')  :  </S_4>  :  0.507575757576\n",
      "(u'start', u'here', '</S_4>', '</S_3>', '</S_2>')  :  </S_1>  :  0.507575757576\n",
      "('<S_2>', '<S_3>', '<S_4>', u'Let', u'us')  :  start  :  0.507575757576\n",
      "(u'here', '</S_4>', '</S_3>', '</S_2>', '</S_1>')  :  </S_0>  :  0.507575757576\n",
      "('<S_4>', u'Why', u'dont', u'we', u'start')  :  here  :  0.917297979798\n",
      "('<S_2>', '<S_3>', '<S_4>', u'Why', u'dont')  :  we  :  0.671717171717\n",
      "(u'we', u'end', u'there', '</S_4>', '</S_3>')  :  </S_2>  :  0.507575757576\n",
      "(u'start', u'with', u'a', u'few', u'other')  :  examples  :  0.969696969697\n",
      "(u'dont', u'we', u'end', u'there', '</S_4>')  :  </S_3>  :  0.507575757576\n",
      "(u'with', u'an', u'example', u'with', u'so')  :  few  :  0.970170454545\n",
      "(u'Why', u'dont', u'we', u'end', u'there')  :  </S_4>  :  0.507575757576\n",
      "(u'an', u'example', u'with', u'so', u'few')  :  tokens  :  0.954071969697\n",
      "(u'us', u'start', u'with', u'a', u'few')  :  other  :  0.954071969697\n",
      "('<S_3>', '<S_4>', u'We', u'never', u'start')  :  with  :  0.507575757576\n",
      "(u'Let', u'us', u'start', u'with', u'a')  :  few  :  0.970170454545\n",
      "(u'tokens', '</S_4>', '</S_3>', '</S_2>', '</S_1>')  :  </S_0>  :  0.507575757576\n",
      "('<S_1>', '<S_2>', '<S_3>', '<S_4>', u'We')  :  never  :  0.507575757576\n",
      "('<S_1>', '<S_2>', '<S_3>', '<S_4>', u'Why')  :  dont  :  0.671717171717\n",
      "('<S_1>', '<S_2>', '<S_3>', '<S_4>', u'Let')  :  us  :  0.507575757576\n",
      "(u'can', u'be', u'words', u'that', u'we')  :  start  :  0.964393939394\n",
      "('<S_4>', u'Let', u'us', u'start', u'with')  :  a  :  0.90459280303\n",
      "(u'be', u'words', u'that', u'we', u'start')  :  with  :  0.939078282828\n",
      "(u'in', u'example', u'docs', '</S_4>', '</S_3>')  :  </S_2>  :  0.507575757576\n",
      "('<S_3>', '<S_4>', u'Tokens', u'can', u'be')  :  words  :  0.507575757576\n",
      "(u'words', u'that', u'we', u'start', u'with')  :  in  :  0.90459280303\n",
      "(u'never', u'start', u'with', u'an', u'example')  :  with  :  0.955492424242\n",
      "(u'Tokens', u'can', u'be', u'words', u'that')  :  we  :  0.970643939394\n",
      "(u'dont', u'we', u'start', u'here', '</S_4>')  :  </S_3>  :  0.507575757576\n",
      "('<S_3>', '<S_4>', u'Let', u'us', u'start')  :  with  :  0.507575757576\n",
      "(u'few', u'other', u'examples', '</S_4>', '</S_3>')  :  </S_2>  :  0.507575757576\n",
      "(u'docs', '</S_4>', '</S_3>', '</S_2>', '</S_1>')  :  </S_0>  :  0.507575757576\n",
      "(u'with', u'so', u'few', u'tokens', '</S_4>')  :  </S_3>  :  0.507575757576\n",
      "('<S_2>', '<S_3>', '<S_4>', u'We', u'never')  :  start  :  0.507575757576\n",
      "(u'We', u'never', u'start', u'with', u'an')  :  example  :  0.970170454545\n",
      "(u'example', u'with', u'so', u'few', u'tokens')  :  </S_4>  :  0.507575757576\n",
      "(u'start', u'with', u'in', u'example', u'docs')  :  </S_4>  :  0.507575757576\n",
      "('<S_4>', u'Why', u'dont', u'we', u'end')  :  there  :  0.969696969697\n",
      "(u'example', u'docs', '</S_4>', '</S_3>', '</S_2>')  :  </S_1>  :  0.507575757576\n",
      "(u'there', '</S_4>', '</S_3>', '</S_2>', '</S_1>')  :  </S_0>  :  0.507575757576\n",
      "(u'with', u'in', u'example', u'docs', '</S_4>')  :  </S_3>  :  0.507575757576\n",
      "(u'that', u'we', u'start', u'with', u'in')  :  example  :  0.970170454545\n",
      "(u'start', u'with', u'an', u'example', u'with')  :  so  :  0.946259469697\n",
      "(u'few', u'tokens', '</S_4>', '</S_3>', '</S_2>')  :  </S_1>  :  0.507575757576\n",
      "(u'end', u'there', '</S_4>', '</S_3>', '</S_2>')  :  </S_1>  :  0.507575757576\n",
      "('<S_1>', '<S_2>', '<S_3>', '<S_4>', u'Tokens')  :  can  :  0.507575757576\n",
      "(u'other', u'examples', '</S_4>', '</S_3>', '</S_2>')  :  </S_1>  :  0.507575757576\n",
      "('<S_4>', u'Tokens', u'can', u'be', u'words')  :  that  :  0.969696969697\n",
      "(u'so', u'few', u'tokens', '</S_4>', '</S_3>')  :  </S_2>  :  0.507575757576\n",
      "(u'examples', '</S_4>', '</S_3>', '</S_2>', '</S_1>')  :  </S_0>  :  0.507575757576\n",
      "(u'we', u'start', u'here', '</S_4>', '</S_3>')  :  </S_2>  :  0.507575757576\n",
      "('<S_3>', '<S_4>', u'Why', u'dont', u'we')  :  start  :  0.257575757576\n",
      "('<S_3>', '<S_4>', u'Why', u'dont', u'we')  :  end  :  0.257575757576\n",
      "(u'a', u'few', u'other', u'examples', '</S_4>')  :  </S_3>  :  0.507575757576\n",
      "Total Probability:  37.7575862795\n",
      "Count:  61\n"
     ]
    }
   ],
   "source": [
    "#sx = tokenize(x)\n",
    "\n",
    "m = wb_model_builder(small_tokens, 6)\n",
    "model_printer(m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Self assessment:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = ('a','b','c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tup = tup[1:]\n",
    "tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
