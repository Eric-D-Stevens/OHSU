{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters: Homework 2\n",
    "### Eric D. Stevens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Trends Over Time\n",
    "\n",
    "<img src=\"./images/over_time.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collection\n",
    "\n",
    "The code for this segment can be found in `hw2/q1/q1.py`. \n",
    "\n",
    "The process I used to gather this data was straight forward. First and RDD was generated directly from the Hadoop directory. A `rdd.map()` was used to split each line and only collect the dates, check to see that the date could be parsed, and then write the date as a string as the only output. A try clause was used to do this so that any bad columns would be filtered out by an `rdd.filter()` method following the map stage. The code for the mapper can be seen here:\n",
    "\n",
    "```python\n",
    "# mapping funciton\n",
    "def get_day_only(line):\n",
    "    fail = line\n",
    "    try:\n",
    "        line = line.split('\\t')[0] # only look at first column\n",
    "        line = line.split()[0] # remove time\n",
    "        line = line.split('-') # split date yyyy mm dd\n",
    "        date(int(line[0]), int(line[1]), int(line[2]))\n",
    "        line = '-'.join(line)\n",
    "    except:\n",
    "        print(fail)\n",
    "        line = False # for filtering later\n",
    "    return line\n",
    "```\n",
    "\n",
    "After passing the data through the map and the filter, what was left was a single column of dates. I then used the `rdd.countByValue()` function to accumulate the counts by day. I then used pickle to save the dictionary returned by this function to file for image building off of the cluster.\n",
    "\n",
    "#### Figure Replication\n",
    "\n",
    "The code for this segment can be found in `hw2/q1/hw2q1_plot.ipynb`. \n",
    "\n",
    "To replicate the figure found in the paper I had to create an axis that had a value for every day between the first query date in the data and the last. To do this I created a range of dates and mapped data retrieved in spark to their place on the range, leaving all non existing days as zeros. The code is below:\n",
    "\n",
    "```python\n",
    "# date range\n",
    "current = min(date_dict.keys())\n",
    "stop = max(date_dict.keys())\n",
    "step = timedelta(days=1)\n",
    "\n",
    "#create range\n",
    "x_day = []\n",
    "y_val = []\n",
    "\n",
    "#month range\n",
    "months = []\n",
    "m_split = []\n",
    "while current <= stop:\n",
    "    x_day.append(current)\n",
    "    y_val.append(date_dict[current])\n",
    "    if current.day == 15:\n",
    "        months.append(current)\n",
    "    if current.day == 1:\n",
    "        m_split.append(current)\n",
    "    current += step\n",
    "```\n",
    "\n",
    "After the values were properly mapped onto the date axis, the figure could be constructed. I will spare the english details and let the code speak:\n",
    "\n",
    "```python\n",
    "fig = plt.figure(figsize=(18,3))\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.ticklabel_format(axis='y', style='sci')\n",
    "\n",
    "plt.suptitle('Sci-Hub activity over 6 months', x=.198, y=1.12, fontsize='xx-large', fontweight='bold')\n",
    "plt.figtext(x=0.08, y=.98, s='Sci-Hubs domain switch in November 2015, forced by a lawsuit against it, ' \\\n",
    "                            + 'led to some missing data during the 6-month period, but usage hit ' \\\n",
    "                            + 'record levels in February.')\n",
    "\n",
    "\n",
    "plt.fill_between(x_day, y_val, 0, facecolor='#800000')\n",
    "plt.ylabel('NUMBER OF DOWNLOADS')\n",
    "plt.xlim((x_day[0],x_day[-2]))\n",
    "plt.ylim((0,300000))\n",
    "plt.xticks(months, ['SEPTEMBER','OCTOBER','NOVEMBER','DECEMBER','JANUARY','FEBRUARY']);\n",
    "\n",
    "for i in m_split:\n",
    "    ax.axvline(i, color='grey', linestyle='--')\n",
    "for mid_month in months:\n",
    "    ax.text(mid_month, 290000, 'Total: {}'.format(month_count[mid_month.month]), horizontalalignment='center')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Most Downloaded Papers\n",
    "\n",
    "<img src=\"./images/q2_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "code: `./hw2/q2/p2.py`\n",
    "\n",
    "In this section, I used the exact same methodology as the last, mapping only the article DOI to a single column and then using the `rdd.countByValue()` function to complete the summing.\n",
    "\n",
    "```python\n",
    "# mapping funciton\n",
    "def get_doi_only(line):\n",
    "    fail = line\n",
    "    try:\n",
    "        line = line.split('\\t')[1] # only look at first column\n",
    "    except:\n",
    "        print(fail)\n",
    "        line = False # for filtering later\n",
    "    return line\n",
    "```\n",
    "\n",
    "### API for Table Creation\n",
    "\n",
    "code: `./hw2/q2/hw2q2_table.ipynb.py`\n",
    "\n",
    "The `crossrefapi` library was used to generate citations for the table.\n",
    "\n",
    "```python\n",
    "from crossref.restful import Works\n",
    "works = Works()\n",
    "\n",
    "def citation_generator(doi):\n",
    "\n",
    "    article = ''\n",
    "    record = works.doi(doi)\n",
    "\n",
    "    # Author information\n",
    "    try:\n",
    "        at_el = False\n",
    "        for a in record['author']:\n",
    "            if a['sequence'] == 'first':\n",
    "                article += '{} {}, '.format(a['given'],a['family'])\n",
    "            else:\n",
    "                at_el = True\n",
    "        article = article[:-2]\n",
    "        if at_el:\n",
    "            article = article + ', at el '\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Date and Title\n",
    "    try:\n",
    "        year = record['indexed']['date-time'][:4]\n",
    "        title = record['title'][0]\n",
    "        article += '({}) {}. '.format(year, title)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Publication\n",
    "    try:\n",
    "        article += '{} ({}):'.format(record['publisher'], record['issue'])\n",
    "        article += '{}'.format(record['page'])\n",
    "    except:\n",
    "        pass   \n",
    "    return article\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Journals & Publishers\n",
    "\n",
    "<img src=\"./images/q3_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code: `./hw2/q3/(q3.py, b3.py)`\n",
    "\n",
    "For this section, a two phase approach was taken. First, a similar method to the first two problems was used. The cluster was used to split and count the publishers, this time using a regular expression to obtain only the publisher DOI prefix. The top 10 counts were extracted as in the other problems and then pickled.\n",
    "\n",
    "```python\n",
    "# mapping funciton\n",
    "def get_doi_prefix(line):\n",
    "    fail = line\n",
    "    try:\n",
    "        line = line.split('\\t')[1] # only look at first column\n",
    "        line = re.match('\\d+\\.\\d+/',line).group(0)[:-1]\n",
    "    except:\n",
    "        print('fail')\n",
    "        line = False # for filtering later\n",
    "    return line\n",
    "```\n",
    "\n",
    "After this, a second python script was created to load the counts and broadcast them across the cluster. After the broadcast, the publisher prefixes of the top 10 counts were used to filter the prefix .csv file, resulting in an rdd that only contains publisher information form the journals appearing in the top ten. The results were simply combined by mapping the DOI prefixes to each other.\n",
    "\n",
    "```python\n",
    "# broadcast the dict\n",
    "broad = sc.broadcast(p_cnt_dict).value\n",
    "\n",
    "rdd = sc.textFile(\"file:///l2/corpora/scihub/publisher_DOI_prefixes.csv\")\n",
    "print('hello spark')\n",
    "print(rdd.take(3))\n",
    "\n",
    "print('FILTER >>>>')\n",
    "rdd = rdd.filter(filter_top_10)\n",
    "print('FILTERED >>>>')\n",
    "\n",
    "print('MAP >>>>')\n",
    "rdd = rdd.map(map_count_top_10)\n",
    "print('MAPPED >>>>')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Geography\n",
    "\n",
    "Population Data Source: <a href=\"https://data.opendatasoft.com/explore/dataset/1000-largest-us-cities-by-population-with-geographic-coordinates%40public/export/?sort=-rank\"> OpenDataSoft </a>\n",
    "\n",
    "### 1) Top 20 Countries\n",
    "\n",
    "code: `./hw2/q4/p1/p1.py`\n",
    "\n",
    "The strategy used for the first two sections of question 4 was to download population data and put it in a local directory on the server. For this problem, a dataset was downloaded that contained the population of each of the world's countries in 2015. After using a similar method as earlier in the assignment to count the downloads per country, the population data was broadcasted into spark and used by a mapper to divide the count results by the country's populations. \n",
    "\n",
    "<div>\n",
    "    <img src=\"./images/country_count.png\" style=\"float: left; margin-right: 10%\">\n",
    "    <img src=\"./images/country_rel.png\" style=\"float: left\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) U.S. Cities\n",
    "\n",
    "code: `./hw2/q4/p2/p2.py`\n",
    "\n",
    "For this problem, a dataset was downloaded that contained the population of 1000 US cities. After using a similar method as earlier in the assignment to count the downloads per city, the population data was broadcasted into spark and used by a mapper to divide the count results by the citie's populations. \n",
    "\n",
    "<div>\n",
    "    <img src=\"./images/city_total.png\" style=\"float: left; margin-right: 10%\">\n",
    "    <img src=\"./images/city_capita.png\" style=\"float: left\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) The Portlands\n",
    "\n",
    "code: `./hw2/q4/p3/p3.py`\n",
    "\n",
    "For this final problem several filters were used in order to split the original RDD into a Portland Oregon RDD and a Portland Maine RDD. This was done by first filtering the entire dataset for strings containing “United States” and “Portland”. After obtaining all the “Portlands” of interest, the RDD was split by parsing out and filtering on geographical data in the records. \n",
    "\n",
    "<div>\n",
    "    <img src=\"./images/oregon.png\" style=\"float: left; width: 40%; margin-right: 10%\">\n",
    "    <img src=\"./images/maine.png\" style=\"float: left; width: 40%\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
