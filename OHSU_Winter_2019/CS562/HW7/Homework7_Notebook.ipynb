{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 562: Homework 7\n",
    "### Eric D. Stevens\n",
    "### March 14, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model1\n",
    "import bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: IBM Model 1\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This code should not be run in the notebook, this is here only for demonstration purposes. \n",
    "Please use the modle1.py file to run this code '''\n",
    "\n",
    "class Model1(object):\n",
    "    \"\"\"\n",
    "    IBM Model 1 translation table\n",
    "    \"\"\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "    def __init__(self, source, target):\n",
    "\n",
    "        print('--------------- Building Object ---------------')\n",
    "        print('Source File:', source)\n",
    "        print('Target File:', target)\n",
    "        print()\n",
    "\n",
    "        # add filenames to class\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "        # storage arrays\n",
    "        self.P = defaultdict(lambda: defaultdict(np.float64))\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        # initalize P with counts\n",
    "        with open(self.source) as S_src:\n",
    "            with open(self.target) as  T_src:\n",
    "                #for s in S\n",
    "                gen = bitext(S_src,T_src)\n",
    "                for S,T in gen:\n",
    "                    i += 1 # track progress\n",
    "                    if not i % 10000: print('At pair:',i)\n",
    "                    for s in S:\n",
    "                        for t in T:\n",
    "                            self.P[s][t] += 1.0\n",
    "        print('Total:',i,'pairs\\n')\n",
    "\n",
    "        # Normalization of P for probabilities\n",
    "        Plen = len(self.P)\n",
    "        print('Normalizing over',Plen,'source words.')\n",
    "        for s in self.P:\n",
    "            count = sum(self.P[s].values())\n",
    "            for t in self.P[s]:\n",
    "                self.P[s][t] /= count\n",
    "        print('Initalization Complete')\n",
    "\n",
    "\n",
    "\n",
    "    def train(self, n):\n",
    "        \"\"\"\n",
    "        Perform n iterations of EM training\n",
    "        \"\"\"\n",
    "        print('--------------- Begin Training ---------------\\n')\n",
    "        for _ in range(n):\n",
    "            print('########## Iteration:',_+1, '##########')\n",
    "\n",
    "            # reinitialize each epoch (match slide simple example)\n",
    "            self.a = defaultdict(lambda: defaultdict(np.float64))\n",
    "            self.T = defaultdict(np.float64)\n",
    "\n",
    "            # use file streams\n",
    "            with open(self.source) as S_src:\n",
    "                with open(self.target) as  T_src:\n",
    "\n",
    "                    # generator object\n",
    "                    gen = bitext(S_src,T_src)\n",
    "\n",
    "                    # track progress\n",
    "                    i=0\n",
    "\n",
    "                    # for Source Target sentences in the files\n",
    "                    for S,T in gen:\n",
    "                        i += 1 # track progress\n",
    "                        if not i % 10000: print('At pair:',i)\n",
    "\n",
    "                        # build up a and T\n",
    "                        for s in S:\n",
    "                            for t in T:\n",
    "                                self.a[s][t] += self.P[s][t]\n",
    "                                self.T[t] += self.P[s][t]\n",
    "\n",
    "\n",
    "            # set P(t|s) = a(s,t)/T(t)\n",
    "            print('\\nUpdating P(t|s)')\n",
    "            for s in self.a:\n",
    "                for t in self.a[s]:\n",
    "                    self.P[s][t] = self.a[s][t]/self.T[t]\n",
    "            print('Normalizing over P(t|s)')\n",
    "            for s in self.P:\n",
    "                count = sum(self.P[s].values())\n",
    "                for t in  self.P[s]:\n",
    "                    self.P[s][t] /= count\n",
    "            print()\n",
    "        print('--------------- Training Complete ---------------')\n",
    "\n",
    "\n",
    "    def get_word_translation(self, word):\n",
    "        ''' returns the translation of 'word' '''\n",
    "        key_max = max(self.P[word].keys(), key=(lambda k: self.P[word][k]))\n",
    "        return key_max\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Building the model\n",
    "\n",
    "Here we call the model constructor with the file names of the parrallel text as parameters. Durring the initialization step, maximum likelihood estimation 'P(target_word|source_word)' is calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Building Object ---------------\n",
      "Source File: hansards.36.ca.f.tok\n",
      "Target File: hansards.36.ca.e.tok\n",
      "\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "Total: 207688 pairs\n",
      "\n",
      "Normalizing over 50652 source words.\n",
      "Initalization Complete\n"
     ]
    }
   ],
   "source": [
    "Fr2Eng = model1.Model1('hansards.36.ca.f.tok','hansards.36.ca.e.tok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Here we call the memberfunction 'train' with the number of epochs as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Begin Training ---------------\n",
      "\n",
      "########## Iteration: 1 ##########\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "\n",
      "Updating P(t|s)\n",
      "Normalizing over P(t|s)\n",
      "\n",
      "########## Iteration: 2 ##########\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "\n",
      "Updating P(t|s)\n",
      "Normalizing over P(t|s)\n",
      "\n",
      "########## Iteration: 3 ##########\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "\n",
      "Updating P(t|s)\n",
      "Normalizing over P(t|s)\n",
      "\n",
      "########## Iteration: 4 ##########\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "\n",
      "Updating P(t|s)\n",
      "Normalizing over P(t|s)\n",
      "\n",
      "########## Iteration: 5 ##########\n",
      "At pair: 10000\n",
      "At pair: 20000\n",
      "At pair: 30000\n",
      "At pair: 40000\n",
      "At pair: 50000\n",
      "At pair: 60000\n",
      "At pair: 70000\n",
      "At pair: 80000\n",
      "At pair: 90000\n",
      "At pair: 100000\n",
      "At pair: 110000\n",
      "At pair: 120000\n",
      "At pair: 130000\n",
      "At pair: 140000\n",
      "At pair: 150000\n",
      "At pair: 160000\n",
      "At pair: 170000\n",
      "At pair: 180000\n",
      "At pair: 190000\n",
      "At pair: 200000\n",
      "\n",
      "Updating P(t|s)\n",
      "Normalizing over P(t|s)\n",
      "\n",
      "--------------- Training Complete ---------------\n"
     ]
    }
   ],
   "source": [
    "Fr2Eng.train(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing translation\n",
    "\n",
    "Here we use the member function 'get_word_translation()' with a source word as a parameter. This function will return the translation of the source word based on the model. If the modle has been initalized but not been trained, the function will return the maximum likelihood estimation.\n",
    "\n",
    "The usage of the function on its own is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMMITTEE'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fr2Eng.get_word_translation('COMITÉ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following small script reads in a file of source words that are in a single word per line format. The script translates these words, wirtes the pairs of words to std out, and writes the pairs to the file 'f2ewords.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFAIRES            AFFAIRS             \n",
      "AMENDEMENT          AMENDMENT           \n",
      "AUTOCHTONES         ABORIGINALS         \n",
      "CHAMBRE             HOUSE               \n",
      "CHEF                CHIEF               \n",
      "COMITÉ              COMMITTEE           \n",
      "COMMERCE            TRADE               \n",
      "COMMUNES            COMMONS             \n",
      "DROITS              RIGHTS              \n",
      "DÉCLARATION         STATEMENT           \n",
      "DÉFENSE             DEFENCE             \n",
      "DÉVELOPPEMENT       DEVELOPMENT         \n",
      "ENFANTS             CHILDREN            \n",
      "FINANCES            FINANCE             \n",
      "FÉDÉRAL             FEDERAL             \n",
      "GENS                PEOPLE              \n",
      "GOUVERNEMENT        GOVERNMENT          \n",
      "GUERRE              WAR                 \n",
      "HISTOIRE            HISTORY             \n",
      "INTERNATIONALE      INTERNATIONALE      \n",
      "JUGE                JUDGE               \n",
      "MINISTÈRE           DEPARTMENT          \n",
      "MONDE               WORLD               \n",
      "NATIONALE           NATIONAL            \n",
      "PARLEMENT           PARLIAMENT          \n",
      "PAROLE              SPEAK               \n",
      "PARTIE              PART                \n",
      "PREMIER             PRIME               \n",
      "PREMIÈRE            FIRST               \n",
      "PROGRAMME           PROGRAM             \n",
      "PROJET              PROJECT             \n",
      "PROVINCE            PROVINCE            \n",
      "PRÉSIDENT           PRESIDENT           \n",
      "QUÉBEC              QUEBEC              \n",
      "RAISON              REASON              \n",
      "RAPPORT             REPORT              \n",
      "RESPONSABILITÉ      RESPONSIBILITY      \n",
      "RÉGIME              REGIME              \n",
      "RÉGION              REGION              \n",
      "RÉPONSE             ANSWER              \n",
      "SECTEUR             SECTOR              \n",
      "SERVICES            SERVICES            \n",
      "SOCIÉTÉ             SOCIETY             \n",
      "SÉCURITÉ            SECURITY            \n",
      "SÉNAT               SENATE              \n",
      "SÉNATEUR            SENATOR             \n",
      "TRAVAIL             LABOUR              \n",
      "ÉGALEMENT           EQUALLY             \n",
      "ÉTATS-UNIS          U.S.                \n",
      "ÉTUDE               STUDY               \n"
     ]
    }
   ],
   "source": [
    "# Turn file into word list (one word per line).\n",
    "french_word_list = [line.rstrip() for line in open('fwords.txt')]\n",
    "\n",
    "# Make translated word list using member function\n",
    "english_word_list = [Fr2Eng.get_word_translation(fr_word) for fr_word in french_word_list]\n",
    "\n",
    "# Print translations and write to file\n",
    "with open('f2ewords.txt', 'w+') as f2e:\n",
    "    for i in range(len(french_word_list)):\n",
    "        two_words = '{:20s}{:20s}'.format(french_word_list[i], english_word_list[i])\n",
    "        print(two_words)\n",
    "        f2e.writelines('%s\\n'%two_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "I am very impressed at how well this method works for single word translations. I found it to be simple and elegant. My approach involved using default dicts to build weighted counts and the normalizing those counts on each training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bleu Evaluation\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This code should not be run in the notebook, this is here only for demonstration purposes. \n",
    "Please use the bleu.py file to run this code '''\n",
    "\n",
    "\n",
    "def BLEU(hypothesis, reference, n=MAX_GRAM_SIZE):\n",
    "    \"\"\"\n",
    "    Compute BLEU for a hypothesis/reference sentence pair\n",
    "    \"\"\"\n",
    "    print(\"--------------- BLEU TRANSLATION SCORE ---------------\\n\")\n",
    "    print(\"Candidate file:\",hypothesis)\n",
    "    print(\"Reference file:\",reference)\n",
    "    print()\n",
    "\n",
    "    out_f_name = hypothesis+'.to.'+reference+'.bleu_scores'\n",
    "    with open(out_f_name, 'w+') as out_file:\n",
    "        out_file.write(\"--------------- BLEU TRANSLATION SCORE ---------------\\n\")\n",
    "        out_file.write(\"Candidate file: \"+hypothesis+\"\\n\")\n",
    "        out_file.write(\"Reference file: \"+reference+\"\\n\")\n",
    "\n",
    "\n",
    "        with open(hypothesis) as cnd:\n",
    "            with open(reference) as ref:\n",
    "                gen = bitext(cnd, ref)\n",
    "\n",
    "                sentence = 0\n",
    "                for c, r in gen:\n",
    "\n",
    "                    # sentence number\n",
    "                    sentence+=1\n",
    "\n",
    "                    # holds list\n",
    "                    pn_list = []\n",
    "\n",
    "                    # all grams for all ns in r\n",
    "                    for nn in range(1,n+1):\n",
    "\n",
    "                        # counters for grams\n",
    "                        c_counts = Counter()\n",
    "                        r_counts = Counter()\n",
    "\n",
    "                        # count target grams\n",
    "                        grm_gen = ngrams(r,nn)\n",
    "                        for gm in grm_gen:\n",
    "                            r_counts[gm] += 1\n",
    "\n",
    "                        # modified count of source grams\n",
    "                        grm_gen = ngrams(c,nn)\n",
    "                        for gm in grm_gen:\n",
    "                            if c_counts[gm]<r_counts[gm]:\n",
    "                                c_counts[gm] += 1\n",
    "\n",
    "                        # calc Pn from n-gram\n",
    "                        pn_list.append(float(sum(c_counts.values()))/float(len(c)))\n",
    "\n",
    "\n",
    "                    # geometric mean of 1-gram to n-gram\n",
    "                    geo_mean = 1.0\n",
    "                    for x in pn_list:\n",
    "                        geo_mean *= x\n",
    "                    geo_mean = pow(geo_mean, 1.0/float(MAX_GRAM_SIZE))\n",
    "\n",
    "                    # final computation\n",
    "                    BP = 1.0\n",
    "                    if len(c) <= len(r):\n",
    "                        BP = exp(1-(float(len(r))/float(len(c))))\n",
    "\n",
    "\n",
    "                    print('Sentence', sentence, ':', BP*geo_mean)\n",
    "                    out_file.write('Sentence'+str(sentence)+':'+str(BP*geo_mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an evaluation on a candidate file and a reference file\n",
    "\n",
    "To run the evaluation on the a candidate file with a reference file just use the `BLEU` function with the candidate and refence files as parameters respectivly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- BLEU TRANSLATION SCORE ---------------\n",
      "\n",
      "Candidate file: gtranslate.tok\n",
      "Reference file: e.tok\n",
      "\n",
      "Sentence 1 : 0.5317417453075524\n",
      "Sentence 2 : 0.26770931469758097\n",
      "Sentence 3 : 0.0\n",
      "Sentence 4 : 0.38071015414152043\n",
      "Sentence 5 : 0.3164918233231852\n",
      "Sentence 6 : 0.7623917462370566\n",
      "Sentence 7 : 0.21188540235493908\n",
      "Sentence 8 : 0.36156345106488474\n",
      "Sentence 9 : 0.5120035191736744\n",
      "Sentence 10 : 0.6017728944812497\n"
     ]
    }
   ],
   "source": [
    "reload(bleu)\n",
    "bleu.BLEU('gtranslate.tok','e.tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- BLEU TRANSLATION SCORE ---------------\n",
      "\n",
      "Candidate file: systran.tok\n",
      "Reference file: e.tok\n",
      "\n",
      "Sentence 1 : 0.3600776854897997\n",
      "Sentence 2 : 0.2691662243112666\n",
      "Sentence 3 : 0.0\n",
      "Sentence 4 : 0.29730177875068026\n",
      "Sentence 5 : 0.24062491113147336\n",
      "Sentence 6 : 0.7659756237473674\n",
      "Sentence 7 : 0.40745500016423886\n",
      "Sentence 8 : 0.3696853394838423\n",
      "Sentence 9 : 0.33635856610148585\n",
      "Sentence 10 : 0.0\n"
     ]
    }
   ],
   "source": [
    "bleu.BLEU('systran.tok','e.tok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts\n",
    "\n",
    "The Bleu scores came out as expected. All of the scores are in between 1 and 0 as specified in the paper. One interesting outcome of the way that the score is calculated, I found, is that it seems that if there is ever an instance where no ngrams of a high order an be found, it forces the score of the entire gram to zero. This comes form the fact that we are using a geometric mean to average.\n",
    "\n",
    "With the geometric mean, I found that it became problimatic to use the equation in the paper since there were instances where one would need to evaluate the log of 0. I solved thes by taking the product of all of the modified ngram percisions and taking the value to the power of one over the order of the highest ngram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
