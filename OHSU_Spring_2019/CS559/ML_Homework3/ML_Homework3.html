<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=cGvuclDC_Z1vE_cnVEU6Ae_NZQ7StBcqH_vXVqoPMX0');ol.lst-kix_a0vn9crd1ry7-7{list-style-type:none}ol.lst-kix_a0vn9crd1ry7-8{list-style-type:none}.lst-kix_wfic13ywmzwj-4>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-4,lower-latin) ". "}.lst-kix_wfic13ywmzwj-5>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-5,lower-roman) ". "}ol.lst-kix_a0vn9crd1ry7-5{list-style-type:none}ol.lst-kix_wfic13ywmzwj-8.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-8 0}ol.lst-kix_a0vn9crd1ry7-6{list-style-type:none}.lst-kix_wfic13ywmzwj-3>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-3,decimal) ". "}.lst-kix_wfic13ywmzwj-7>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-7,lower-latin) ". "}.lst-kix_x2y7t5ebwmmw-2>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-2}.lst-kix_wfic13ywmzwj-0>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-0,decimal) ". "}.lst-kix_wfic13ywmzwj-1>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-1,lower-latin) ". "}.lst-kix_wfic13ywmzwj-8>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-8,lower-roman) ". "}.lst-kix_3rivd9pknt7m-0>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-0}.lst-kix_bdtrs3leuhv8-2>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-2}.lst-kix_wfic13ywmzwj-2>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-2,lower-roman) ". "}.lst-kix_wfic13ywmzwj-2>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-2}.lst-kix_ja603hilijz3-6>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-6,decimal) ". "}.lst-kix_ja603hilijz3-0>li{counter-increment:lst-ctn-kix_ja603hilijz3-0}.lst-kix_ja603hilijz3-4>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-4,lower-latin) ". "}.lst-kix_ja603hilijz3-8>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-8,lower-roman) ". "}.lst-kix_ja603hilijz3-3>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-3,decimal) ". "}.lst-kix_ja603hilijz3-7>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-7,lower-latin) ". "}ol.lst-kix_a0vn9crd1ry7-3.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-3 0}ol.lst-kix_bdtrs3leuhv8-5.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-5 0}.lst-kix_ja603hilijz3-5>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-5,lower-roman) ". "}ol.lst-kix_ja603hilijz3-3.start{counter-reset:lst-ctn-kix_ja603hilijz3-3 0}ol.lst-kix_bdtrs3leuhv8-0.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-0 0}.lst-kix_x2y7t5ebwmmw-1>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-1,lower-latin) ". "}.lst-kix_x2y7t5ebwmmw-2>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-2,lower-roman) ". "}.lst-kix_ja603hilijz3-0>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-0,decimal) ". "}.lst-kix_x2y7t5ebwmmw-3>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-3,decimal) ". "}.lst-kix_x2y7t5ebwmmw-0>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-0,decimal) ". "}ol.lst-kix_x2y7t5ebwmmw-8.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-8 0}.lst-kix_wfic13ywmzwj-4>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-4}.lst-kix_ja603hilijz3-2>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-2,lower-roman) ". "}ol.lst-kix_3rivd9pknt7m-3.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-3 0}.lst-kix_ja603hilijz3-1>li:before{content:"" counter(lst-ctn-kix_ja603hilijz3-1,lower-latin) ". "}.lst-kix_bdtrs3leuhv8-0>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-0}.lst-kix_x2y7t5ebwmmw-8>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-8,lower-roman) ". "}.lst-kix_x2y7t5ebwmmw-7>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-7,lower-latin) ". "}.lst-kix_ja603hilijz3-2>li{counter-increment:lst-ctn-kix_ja603hilijz3-2}.lst-kix_x2y7t5ebwmmw-5>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-5,lower-roman) ". "}.lst-kix_x2y7t5ebwmmw-4>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-4,lower-latin) ". "}.lst-kix_x2y7t5ebwmmw-6>li:before{content:"" counter(lst-ctn-kix_x2y7t5ebwmmw-6,decimal) ". "}.lst-kix_a0vn9crd1ry7-5>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-5}.lst-kix_wfic13ywmzwj-6>li:before{content:"" counter(lst-ctn-kix_wfic13ywmzwj-6,decimal) ". "}.lst-kix_x2y7t5ebwmmw-0>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-0}ol.lst-kix_bdtrs3leuhv8-6.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-6 0}ol.lst-kix_wfic13ywmzwj-3.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-3 0}.lst-kix_wfic13ywmzwj-0>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-0}.lst-kix_a0vn9crd1ry7-3>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-3}ol.lst-kix_3rivd9pknt7m-8{list-style-type:none}ol.lst-kix_3rivd9pknt7m-7{list-style-type:none}ol.lst-kix_3rivd9pknt7m-2{list-style-type:none}ol.lst-kix_3rivd9pknt7m-1{list-style-type:none}ol.lst-kix_3rivd9pknt7m-0{list-style-type:none}ol.lst-kix_3rivd9pknt7m-6{list-style-type:none}ol.lst-kix_3rivd9pknt7m-5{list-style-type:none}ol.lst-kix_3rivd9pknt7m-4{list-style-type:none}ol.lst-kix_3rivd9pknt7m-3{list-style-type:none}ol.lst-kix_a0vn9crd1ry7-4.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-4 0}.lst-kix_bdtrs3leuhv8-4>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-4}ol.lst-kix_ja603hilijz3-8.start{counter-reset:lst-ctn-kix_ja603hilijz3-8 0}ol.lst-kix_3rivd9pknt7m-8.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-8 0}ol.lst-kix_wfic13ywmzwj-2.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-2 0}.lst-kix_x2y7t5ebwmmw-4>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-4}ol.lst-kix_a0vn9crd1ry7-0{list-style-type:none}ol.lst-kix_3rivd9pknt7m-2.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-2 0}ol.lst-kix_bdtrs3leuhv8-1.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-1 0}ol.lst-kix_a0vn9crd1ry7-3{list-style-type:none}ol.lst-kix_a0vn9crd1ry7-4{list-style-type:none}ol.lst-kix_a0vn9crd1ry7-1{list-style-type:none}.lst-kix_wfic13ywmzwj-6>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-6}ol.lst-kix_a0vn9crd1ry7-2{list-style-type:none}.lst-kix_bdtrs3leuhv8-7>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-7,lower-latin) ". "}ol.lst-kix_wfic13ywmzwj-1.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-1 0}.lst-kix_3rivd9pknt7m-5>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-5}.lst-kix_ja603hilijz3-6>li{counter-increment:lst-ctn-kix_ja603hilijz3-6}.lst-kix_bdtrs3leuhv8-1>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-1,lower-latin) ". "}.lst-kix_3rivd9pknt7m-6>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-6}.lst-kix_a0vn9crd1ry7-0>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-0}ol.lst-kix_x2y7t5ebwmmw-4.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-4 0}ol.lst-kix_ja603hilijz3-2.start{counter-reset:lst-ctn-kix_ja603hilijz3-2 0}ol.lst-kix_x2y7t5ebwmmw-6{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-5{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-8{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-7{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-2{list-style-type:none}.lst-kix_x2y7t5ebwmmw-6>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-6}ol.lst-kix_x2y7t5ebwmmw-1{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-1.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-1 0}.lst-kix_3rivd9pknt7m-2>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-2,lower-roman) ". "}ol.lst-kix_x2y7t5ebwmmw-4{list-style-type:none}ol.lst-kix_x2y7t5ebwmmw-3{list-style-type:none}.lst-kix_3rivd9pknt7m-0>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-0,decimal) ". "}.lst-kix_3rivd9pknt7m-8>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-8,lower-roman) ". "}ol.lst-kix_a0vn9crd1ry7-2.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-2 0}.lst-kix_3rivd9pknt7m-6>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-6,decimal) ". "}ol.lst-kix_3rivd9pknt7m-7.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-7 0}.lst-kix_bdtrs3leuhv8-3>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-3,decimal) ". "}ol.lst-kix_x2y7t5ebwmmw-0{list-style-type:none}.lst-kix_3rivd9pknt7m-4>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-4}.lst-kix_bdtrs3leuhv8-6>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-6}.lst-kix_3rivd9pknt7m-4>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-4,lower-latin) ". "}.lst-kix_bdtrs3leuhv8-5>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-5,lower-roman) ". "}ol.lst-kix_3rivd9pknt7m-4.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-4 0}.lst-kix_a0vn9crd1ry7-8>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-8}ol.lst-kix_x2y7t5ebwmmw-2.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-2 0}ol.lst-kix_ja603hilijz3-1{list-style-type:none}ol.lst-kix_wfic13ywmzwj-7{list-style-type:none}ol.lst-kix_ja603hilijz3-0{list-style-type:none}.lst-kix_bdtrs3leuhv8-5>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-5}ol.lst-kix_wfic13ywmzwj-6{list-style-type:none}ol.lst-kix_ja603hilijz3-3{list-style-type:none}ol.lst-kix_ja603hilijz3-2{list-style-type:none}.lst-kix_ja603hilijz3-8>li{counter-increment:lst-ctn-kix_ja603hilijz3-8}ol.lst-kix_wfic13ywmzwj-8{list-style-type:none}ol.lst-kix_wfic13ywmzwj-3{list-style-type:none}ol.lst-kix_wfic13ywmzwj-2{list-style-type:none}ol.lst-kix_wfic13ywmzwj-5{list-style-type:none}ol.lst-kix_wfic13ywmzwj-4{list-style-type:none}ol.lst-kix_ja603hilijz3-4.start{counter-reset:lst-ctn-kix_ja603hilijz3-4 0}ol.lst-kix_wfic13ywmzwj-1{list-style-type:none}ol.lst-kix_wfic13ywmzwj-0{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-0{list-style-type:none}.lst-kix_wfic13ywmzwj-5>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-5}ol.lst-kix_bdtrs3leuhv8-7{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-8{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-5{list-style-type:none}.lst-kix_x2y7t5ebwmmw-5>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-5}.lst-kix_ja603hilijz3-7>li{counter-increment:lst-ctn-kix_ja603hilijz3-7}ol.lst-kix_bdtrs3leuhv8-6{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-3{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-4{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-1{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-2{list-style-type:none}.lst-kix_ja603hilijz3-1>li{counter-increment:lst-ctn-kix_ja603hilijz3-1}ol.lst-kix_3rivd9pknt7m-5.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-5 0}.lst-kix_a0vn9crd1ry7-7>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-7}ol.lst-kix_x2y7t5ebwmmw-3.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-3 0}ol.lst-kix_ja603hilijz3-5.start{counter-reset:lst-ctn-kix_ja603hilijz3-5 0}ol.lst-kix_a0vn9crd1ry7-1.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-1 0}.lst-kix_a0vn9crd1ry7-1>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-1}ol.lst-kix_ja603hilijz3-5{list-style-type:none}ol.lst-kix_ja603hilijz3-4{list-style-type:none}ol.lst-kix_ja603hilijz3-7{list-style-type:none}ol.lst-kix_ja603hilijz3-6{list-style-type:none}ol.lst-kix_ja603hilijz3-8{list-style-type:none}ol.lst-kix_bdtrs3leuhv8-8.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-8 0}.lst-kix_a0vn9crd1ry7-6>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-6}.lst-kix_x2y7t5ebwmmw-1>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-1}ol.lst-kix_a0vn9crd1ry7-0.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-0 0}.lst-kix_bdtrs3leuhv8-1>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-1}ol.lst-kix_3rivd9pknt7m-6.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-6 0}.lst-kix_3rivd9pknt7m-1>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-1}.lst-kix_a0vn9crd1ry7-8>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-8,lower-roman) ". "}.lst-kix_wfic13ywmzwj-1>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-1}ol.lst-kix_a0vn9crd1ry7-6.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-6 0}ol.lst-kix_3rivd9pknt7m-0.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-0 0}.lst-kix_a0vn9crd1ry7-6>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-6,decimal) ". "}.lst-kix_a0vn9crd1ry7-7>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-7,lower-latin) ". "}ol.lst-kix_ja603hilijz3-6.start{counter-reset:lst-ctn-kix_ja603hilijz3-6 0}.lst-kix_a0vn9crd1ry7-1>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-1,lower-latin) ". "}.lst-kix_bdtrs3leuhv8-3>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-3}.lst-kix_a0vn9crd1ry7-0>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-0,decimal) ". "}.lst-kix_a0vn9crd1ry7-2>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-2,lower-roman) ". "}ol.lst-kix_x2y7t5ebwmmw-5.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-5 0}ol.lst-kix_bdtrs3leuhv8-3.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-3 0}.lst-kix_a0vn9crd1ry7-5>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-5,lower-roman) ". "}.lst-kix_a0vn9crd1ry7-4>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-4,lower-latin) ". "}.lst-kix_x2y7t5ebwmmw-3>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-3}.lst-kix_a0vn9crd1ry7-3>li:before{content:"" counter(lst-ctn-kix_a0vn9crd1ry7-3,decimal) ". "}ol.lst-kix_wfic13ywmzwj-0.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-0 0}ol.lst-kix_a0vn9crd1ry7-5.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-5 0}.lst-kix_a0vn9crd1ry7-2>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-2}ol.lst-kix_ja603hilijz3-0.start{counter-reset:lst-ctn-kix_ja603hilijz3-0 0}.lst-kix_3rivd9pknt7m-3>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-3}.lst-kix_ja603hilijz3-3>li{counter-increment:lst-ctn-kix_ja603hilijz3-3}ol.lst-kix_3rivd9pknt7m-1.start{counter-reset:lst-ctn-kix_3rivd9pknt7m-1 0}ol.lst-kix_bdtrs3leuhv8-2.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-2 0}ol.lst-kix_ja603hilijz3-7.start{counter-reset:lst-ctn-kix_ja603hilijz3-7 0}ol.lst-kix_bdtrs3leuhv8-7.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-7 0}.lst-kix_bdtrs3leuhv8-7>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-7}ol.lst-kix_x2y7t5ebwmmw-6.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-6 0}.lst-kix_x2y7t5ebwmmw-7>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-7}ol.lst-kix_ja603hilijz3-1.start{counter-reset:lst-ctn-kix_ja603hilijz3-1 0}ol.lst-kix_x2y7t5ebwmmw-0.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-0 0}.lst-kix_wfic13ywmzwj-3>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-3}.lst-kix_bdtrs3leuhv8-8>li{counter-increment:lst-ctn-kix_bdtrs3leuhv8-8}.lst-kix_bdtrs3leuhv8-8>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-8,lower-roman) ". "}ol.lst-kix_bdtrs3leuhv8-4.start{counter-reset:lst-ctn-kix_bdtrs3leuhv8-4 0}ol.lst-kix_x2y7t5ebwmmw-7.start{counter-reset:lst-ctn-kix_x2y7t5ebwmmw-7 0}.lst-kix_bdtrs3leuhv8-0>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-0,decimal) ". "}.lst-kix_wfic13ywmzwj-8>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-8}.lst-kix_x2y7t5ebwmmw-8>li{counter-increment:lst-ctn-kix_x2y7t5ebwmmw-8}ol.lst-kix_wfic13ywmzwj-4.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-4 0}.lst-kix_3rivd9pknt7m-1>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-1,lower-latin) ". "}.lst-kix_wfic13ywmzwj-7>li{counter-increment:lst-ctn-kix_wfic13ywmzwj-7}.lst-kix_3rivd9pknt7m-3>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-3,decimal) ". "}.lst-kix_3rivd9pknt7m-7>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-7}ol.lst-kix_wfic13ywmzwj-7.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-7 0}.lst-kix_bdtrs3leuhv8-2>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-2,lower-roman) ". "}.lst-kix_3rivd9pknt7m-7>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-7,lower-latin) ". "}.lst-kix_3rivd9pknt7m-5>li:before{content:"" counter(lst-ctn-kix_3rivd9pknt7m-5,lower-roman) ". "}.lst-kix_ja603hilijz3-5>li{counter-increment:lst-ctn-kix_ja603hilijz3-5}.lst-kix_bdtrs3leuhv8-4>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-4,lower-latin) ". "}.lst-kix_bdtrs3leuhv8-6>li:before{content:"" counter(lst-ctn-kix_bdtrs3leuhv8-6,decimal) ". "}.lst-kix_3rivd9pknt7m-2>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-2}.lst-kix_3rivd9pknt7m-8>li{counter-increment:lst-ctn-kix_3rivd9pknt7m-8}ol.lst-kix_wfic13ywmzwj-6.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-6 0}ol.lst-kix_a0vn9crd1ry7-8.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-8 0}.lst-kix_ja603hilijz3-4>li{counter-increment:lst-ctn-kix_ja603hilijz3-4}.lst-kix_a0vn9crd1ry7-4>li{counter-increment:lst-ctn-kix_a0vn9crd1ry7-4}ol.lst-kix_wfic13ywmzwj-5.start{counter-reset:lst-ctn-kix_wfic13ywmzwj-5 0}ol.lst-kix_a0vn9crd1ry7-7.start{counter-reset:lst-ctn-kix_a0vn9crd1ry7-7 0}ol{margin:0;padding:0}table td,table th{padding:0}.c15{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:468pt;border-top-color:#000000;border-bottom-style:solid}.c37{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c10{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c14{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c33{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c26{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c21{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c23{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c28{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c32{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c13{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c19{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c29{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c27{color:#434343;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c12{font-family:"Consolas";font-style:italic;color:#999988;font-weight:400}.c30{text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c36{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c18{border-spacing:0;border-collapse:collapse;margin-right:auto}.c35{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{font-family:"Consolas";color:#008080;font-weight:400}.c20{font-family:"Consolas";color:#880000;font-weight:400}.c4{font-family:"Consolas";color:#333333;font-weight:400}.c8{font-family:"Consolas";color:#dd1144;font-weight:400}.c9{font-family:"Consolas";color:#0086b3;font-weight:400}.c16{font-family:"Consolas";color:#444444;font-weight:400}.c17{font-family:"Consolas";color:#333333}.c22{font-family:"Consolas";color:#990000}.c34{padding:0;margin:0}.c25{margin-left:36pt;padding-left:0pt}.c31{color:inherit;text-decoration:inherit}.c6{height:0pt}.c24{font-style:italic}.c1{font-weight:700}.c11{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c35"><p class="c33 title" id="h.et7t00sv3s49"><span class="c37">Machine Learning - Homework 3</span></p><p class="c26 subtitle" id="h.b9s1h3ckddux"><span class="c32">Eric Stevens</span></p><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">This assignment was written in MATLAB R2015b.</span></p><p class="c5 c11"><span class="c2"></span></p><h1 class="c28" id="h.y3ghq5q9d7cv"><span class="c0">Problem 1: Perceptron</span></h1><h2 class="c21" id="h.4cz4y574xhyq"><span class="c29">Description</span></h2><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">This program was written in two parts:</span></p><p class="c5 c11"><span class="c2"></span></p><ol class="c34 lst-kix_3rivd9pknt7m-0 start" start="1"><li class="c5 c25"><span class="c1">perceptron.m </span><span>- This function file takes the raw input data as two seperate classes and returns the ramdomized initializatio of the weight vector, </span><span class="c1">W_init</span><span>, as well as the final learned weights </span><span class="c1">W</span><span>. The function file also has a helper function, </span><span class="c1">converged()</span><span>, that is handed a labeled and biased matrix and a weight vector and tests if the data is properly classified based on the input weight vector. It returns a boolean, </span><span class="c1">has_converged</span><span class="c2">, which is true iff all data points are properly classified and otherwise returns false. </span></li><li class="c5 c25"><span class="c1">problem1.m</span><span class="c2">&nbsp;- This is a very simple script that manually declares the data in their respective classes and then calls the functions detailed above. </span></li></ol><p class="c5 c11"><span class="c2"></span></p><h2 class="c21" id="h.ra7wxz8jqckf"><span class="c29">Question Answers:</span></h2><ol class="c34 lst-kix_x2y7t5ebwmmw-0 start" start="1"><li class="c5 c25"><span class="c1">Is that linearly separable? </span><span class="c2">Yes, we can clearly visualize drawing a line that properly segregates the classes. Furthermore, the fact that we were able to write a linear perceptron learning algorithm that converges implies that the data is linearly seperable.</span></li><li class="c5 c25"><span class="c1">Does the final decision boundary separate all data points correctly? </span><span class="c2">Yes, we can clearly see that in each of the examples below that a solution that completly seperates that data was achieved.</span></li><li class="c5 c25"><span class="c1">&nbsp;How many iterations does it take to converge? </span><span>This depends on both the definition of </span><span class="c24">iterations</span><span>&nbsp;and the random initialization of the weight vector. For my purposes, I have defined iterations as the number of times that the weight vector </span><span class="c24">W</span><span>&nbsp;is updated. Using this definition, there is still no set number of iterations that it is guaranteed to take. This is because the assignment instructs us to randomly initialize the weight vector. In the examples below </span><span class="c1">Example 1</span><span>&nbsp;converges after just 1 update to the weight vector while </span><span class="c1">Example 2</span><span class="c2">&nbsp;takes 18 iterations to converge.</span></li></ol><h2 class="c21" id="h.v51abvrfazqq"><span class="c29">Results</span></h2><p class="c5 c11"><span class="c2"></span></p><h3 class="c10" id="h.3gola790133l"><span class="c19">W = [ Threshold, &nbsp; &nbsp;W1 (x axis), &nbsp; &nbsp;W2 (y axis) ]</span></h3><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.gtc7suflfe3d"><span class="c27 c1">Example Run 1:</span></h3><p class="c5 c11"><span class="c2"></span></p><h3 class="c10" id="h.z8b4cqeaf39w"><span class="c19">W_init = [ 0.8147, &nbsp; &nbsp;0.9058, &nbsp; &nbsp;0.1270 ]</span></h3><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image4.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c10" id="h.c9hgmdlcm9q"><span class="c19">Training Stage</span></h3><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image3.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c10" id="h.92g5y46z842u"><span>W_learned = [ 1.8147, &nbsp; -0.0942, &nbsp; -0.8730 ]</span></h3><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image7.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c13" id="h.z1h0cwl860y9"><span class="c27 c1">Example Run 2:</span></h3><p class="c5 c11"><span class="c2"></span></p><h3 class="c10" id="h.ne3mv8dc73gg"><span>W_init = [ 0.9743, &nbsp; &nbsp;0.3560, &nbsp; &nbsp;0.3253 ]</span></h3><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image8.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c10" id="h.zco8op2uatna"><span class="c19">Training Stage</span></h3><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image1.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h3 class="c10" id="h.bjc7ftp519m5"><span class="c19">W_learned = [ 7.9743, &nbsp; -0.6440, &nbsp; -4.6747]</span></h3><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image5.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c11"><span class="c2"></span></p><h2 class="c21" id="h.tbo3jy81ukgm"><span class="c29">Source</span></h2><h3 class="c13" id="h.r19yqk8b0d5d"><span class="c27 c1">perceptron.m:</span></h3><a id="t.eec13eaa2fc41ae67514e18ac596b42c4901334d"></a><a id="t.0"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">% This function will take an input of two</span><span class="c4"><br></span><span class="c12">% datasets where X1 is class 1 and X2 is</span><span class="c4"><br></span><span class="c12">% class 2. X1 and X2 must have the same </span><span class="c4"><br></span><span class="c12">% number of columns. It will then perform </span><span class="c4"><br></span><span class="c12">% the perceptron learning algorithm. The</span><span class="c4"><br></span><span class="c12">% function will return the final weights</span><span class="c4"><br></span><span class="c12">% as well as the random initial weights.</span><span class="c4"><br></span><span class="c17 c1">function</span><span class="c4">&nbsp;[W, Winit] = </span><span class="c22 c1">perceptron</span><span class="c4">(X1, X2)<br><br> &nbsp; &nbsp;moves = </span><span class="c3">0</span><span class="c4">;<br> &nbsp; &nbsp;</span><span class="c12">% Create seperate and combined class matricies</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c12">% by adding bias in column one and y in column 4</span><span class="c4"><br> &nbsp; &nbsp;C1 = [ones(size(X1,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">), X1, ones(size(X1,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">)]<br> &nbsp; &nbsp;C2 = [ones(size(X2,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">), X2, </span><span class="c3">-1</span><span class="c4">*ones(size(X2,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">)]<br> &nbsp; &nbsp;C = [C1;C2]<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;</span><span class="c12">% get the dimentionality of the training data</span><span class="c4"><br> &nbsp; &nbsp;dim = </span><span class="c9">size</span><span class="c4">(C,</span><span class="c3">2</span><span class="c4">)</span><span class="c3">-1</span><span class="c4">;<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;</span><span class="c12">% initialize random weights</span><span class="c4"><br> &nbsp; &nbsp;W = </span><span class="c9">rand</span><span class="c4">(</span><span class="c3">1</span><span class="c4">, dim);<br> &nbsp; &nbsp;Winit = W; </span><span class="c12">% store initial</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;figure<br> &nbsp; &nbsp;hold on<br> &nbsp; &nbsp;scatter(C1(:,</span><span class="c3">2</span><span class="c4">)&#39;, C1(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;r&#39;</span><span class="c4">);<br> &nbsp; &nbsp;scatter(C2(:,</span><span class="c3">2</span><span class="c4">)&#39;, C2(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;b&#39;</span><span class="c4">);<br> &nbsp; &nbsp;xax = </span><span class="c9">floor</span><span class="c4">(min(C(:,</span><span class="c3">2</span><span class="c4">)))</span><span class="c3">-1</span><span class="c4">: </span><span class="c9">ceil</span><span class="c4">(max(C(:,</span><span class="c3">2</span><span class="c4">)))+</span><span class="c3">1</span><span class="c4">;<br> &nbsp; &nbsp;plot(xax, -(W(</span><span class="c3">2</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">))*xax - (W(</span><span class="c3">1</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)));<br> &nbsp; &nbsp;hold off<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;</span><span class="c12">% initialize plots with class colored data points</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c12">% as well as a line reflecting the initial weights</span><span class="c4"><br> &nbsp; &nbsp;figure<br> &nbsp; &nbsp;hold on<br> &nbsp; &nbsp;scatter(C1(:,</span><span class="c3">2</span><span class="c4">)&#39;, C1(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;r&#39;</span><span class="c4">);<br> &nbsp; &nbsp;scatter(C2(:,</span><span class="c3">2</span><span class="c4">)&#39;, C2(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;b&#39;</span><span class="c4">);<br> &nbsp; &nbsp;xax = </span><span class="c9">floor</span><span class="c4">(min(C(:,</span><span class="c3">2</span><span class="c4">)))</span><span class="c3">-1</span><span class="c4">: </span><span class="c9">ceil</span><span class="c4">(max(C(:,</span><span class="c3">2</span><span class="c4">)))+</span><span class="c3">1</span><span class="c4">;<br> &nbsp; &nbsp;plot(xax, -(W(</span><span class="c3">2</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">))*xax - (W(</span><span class="c3">1</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)));<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;</span><span class="c12">% while learning has not converged</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c17 c1">while</span><span class="c4">&nbsp;~converged(C,W) </span><span class="c12">% helper, see below</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">% for every point in the dataset</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">for</span><span class="c4">&nbsp;</span><span class="c9">i</span><span class="c4">&nbsp;= </span><span class="c3">1</span><span class="c4">:</span><span class="c9">size</span><span class="c4">(C,</span><span class="c3">1</span><span class="c4">);<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">% calculate y hat</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;y = C(</span><span class="c9">i</span><span class="c4">,dim+</span><span class="c3">1</span><span class="c4">);<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;y_hat = y*(W*C(</span><span class="c9">i</span><span class="c4">,</span><span class="c3">1</span><span class="c4">:dim)&#39;);<br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">% update weights</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;lrn_rate = &nbsp;</span><span class="c3">1</span><span class="c4">;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">if</span><span class="c4">&nbsp;y_hat &lt; </span><span class="c3">0</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;moves = moves+</span><span class="c3">1</span><span class="c4">;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;W = W+(y*lrn_rate)*C(</span><span class="c9">i</span><span class="c4">,</span><span class="c3">1</span><span class="c4">:dim);<br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">% plot at each weight update</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;plot(xax, -(W(</span><span class="c3">2</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">))*xax - (W(</span><span class="c3">1</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)));<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">end</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">end</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c17 c1">end</span><span class="c4"><br><br> &nbsp; &nbsp;hold off<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;figure<br> &nbsp; &nbsp;hold on<br> &nbsp; &nbsp;scatter(C1(:,</span><span class="c3">2</span><span class="c4">)&#39;, C1(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;r&#39;</span><span class="c4">);<br> &nbsp; &nbsp;scatter(C2(:,</span><span class="c3">2</span><span class="c4">)&#39;, C2(:,</span><span class="c3">3</span><span class="c4">)&#39;, </span><span class="c8">&#39;filled&#39;</span><span class="c4">, </span><span class="c8">&#39;b&#39;</span><span class="c4">);<br> &nbsp; &nbsp;plot(xax, -(W(</span><span class="c3">2</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">))*xax - (W(</span><span class="c3">1</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)));<br> &nbsp; &nbsp;hold off<br> &nbsp; &nbsp;<br> &nbsp; &nbsp;moves<br></span><span class="c17 c1">end</span><span class="c4"><br><br></span><span class="c12">% This is a helper function for the perceptron</span><span class="c4"><br></span><span class="c12">% function. Handed a data set that includes </span><span class="c4"><br></span><span class="c12">% binary lables in the last column, it will</span><span class="c4"><br></span><span class="c12">% return true if all labels are classifiec </span><span class="c4"><br></span><span class="c12">% correctly and false if they are not.</span><span class="c4"><br></span><span class="c17 c1">function</span><span class="c4">&nbsp;</span><span class="c1 c22">has_converged</span><span class="c4">&nbsp;= </span><span class="c22 c1">converged</span><span class="c4">(C,W)<br><br> &nbsp; &nbsp;</span><span class="c12">% for every data point in set</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c17 c1">for</span><span class="c4">&nbsp;</span><span class="c9">i</span><span class="c4">&nbsp;= </span><span class="c3">1</span><span class="c4">:</span><span class="c9">size</span><span class="c4">(C,</span><span class="c3">1</span><span class="c4">)<br> &nbsp; &nbsp; &nbsp; &nbsp;dim = </span><span class="c9">size</span><span class="c4">(C,</span><span class="c3">2</span><span class="c4">); <br> &nbsp; &nbsp; &nbsp; &nbsp;y = C(</span><span class="c9">i</span><span class="c4">,dim);<br> &nbsp; &nbsp; &nbsp; &nbsp;y_hat = y*(W*C(</span><span class="c9">i</span><span class="c4">,</span><span class="c3">1</span><span class="c4">:dim</span><span class="c3">-1</span><span class="c4">)&#39;);<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c12">% if missclassification</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c1 c17">if</span><span class="c4">&nbsp;y_hat &lt;</span><span class="c3">0</span><span class="c4">;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;has_converged = false;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">return</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c17 c1">end</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c17 c1">end</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c12">% if no missclassification occoured</span><span class="c4"><br> &nbsp; &nbsp;has_converged = true;<br></span><span class="c17 c1">end</span><span class="c4"><br></span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.o2br447si1g9"><span class="c1 c27">problem1.m:</span></h3><a id="t.ce267b4d15d0757110960076d927fa0edeffe750"></a><a id="t.1"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">%% Problem 1</span><span class="c4"><br><br></span><span class="c12">% enter the data manually</span><span class="c4"><br>class1 = [</span><span class="c3">-1</span><span class="c4">&nbsp;</span><span class="c3">-1</span><span class="c4">; </span><span class="c3">2</span><span class="c4">&nbsp;</span><span class="c3">0</span><span class="c4">; </span><span class="c3">2</span><span class="c4">&nbsp;</span><span class="c3">1</span><span class="c4">; </span><span class="c3">0</span><span class="c4">&nbsp;</span><span class="c3">1</span><span class="c4">; </span><span class="c3">0.5</span><span class="c4">&nbsp;</span><span class="c3">1.5</span><span class="c4">];<br>class2 = [</span><span class="c3">3.5</span><span class="c4">&nbsp;</span><span class="c3">2.5</span><span class="c4">; </span><span class="c3">3</span><span class="c4">&nbsp;</span><span class="c3">4</span><span class="c4">; </span><span class="c3">5</span><span class="c4">&nbsp;</span><span class="c3">2</span><span class="c4">; </span><span class="c3">5.5</span><span class="c4">&nbsp;</span><span class="c3">3</span><span class="c4">];<br><br></span><span class="c12">% run perceptron learning algorithm on data</span><span class="c4"><br>[W, Winit] = perceptron(class1, class2)<br><br></span><span class="c12">% calculate slope an intercept (class2 is y axis)</span><span class="c4"><br>slope = -W(</span><span class="c3">2</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)<br>intercept = -W(</span><span class="c3">1</span><span class="c4">)/W(</span><span class="c3">3</span><span class="c4">)</span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><p class="c5 c11"><span class="c2"></span></p><h1 class="c28" id="h.tcky4psbl7o4"><span>Problem 2: Support Vector Machine</span></h1><p class="c5 c11"><span class="c2"></span></p><h2 class="c21" id="h.1fxp4qhe9x54"><span class="c29">Description</span></h2><p class="c5"><span class="c2">Most of the work of this problem is done by the provided functions. The script here simply accomplishes the few tasks that were required in the project description.</span></p><p class="c5 c11"><span class="c2"></span></p><h2 class="c21" id="h.metfxt8739az"><span class="c29">Results</span></h2><p class="c5"><span class="c2">For the provided training and testing split the results are as follows:</span></p><p class="c5 c11"><span class="c2"></span></p><a id="t.258a15a2a01821bcf6abad211c1843363d86cab2"></a><a id="t.2"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c16">training_size = </span><span class="c20">539</span><span class="c16"><br>train_accuracy = </span><span class="c20">0.7681</span><span class="c16"><br>train_error = </span><span class="c20">0.2319</span><span class="c16"><br>train_confusion_matrix =<br> &nbsp; </span><span class="c20">299</span><span class="c16">&nbsp; &nbsp; </span><span class="c20">40</span><span class="c16"><br> &nbsp; &nbsp;</span><span class="c20">85</span><span class="c16">&nbsp; &nbsp;</span><span class="c20">115</span><span class="c16"><br><br><br>testing_size = </span><span class="c20">229</span><span class="c16"><br>test_accuracy = </span><span class="c20">0.8035</span><span class="c16"><br>test_error = </span><span class="c20">0.1965</span><span class="c16"><br>test_confusion_matrix =<br><br> &nbsp; </span><span class="c20">142</span><span class="c16">&nbsp; &nbsp; </span><span class="c20">19</span><span class="c16"><br> &nbsp; &nbsp;</span><span class="c20">26</span><span class="c16">&nbsp; &nbsp; </span><span class="c20">42</span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">These results seemed strange to me since there appears to be a higher performance on the testing data than the training data. I decided to ignore the test train split provided and make my own by shuffling the data. </span></p><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">When shuffling the data and keeping the same size test train split, there appeared to be about a 50% chance that the testing error would be lower than the training error. I then proceeded to make a 9:1 test train split. Doing this there is a much higher likelihood that the training error is less than the testing error. Even with the 90:10 split there were still many appearances of the test data having lower error rates than the training data and I am curious as to why this is.</span></p><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">The code that accomplishes the shuffling and re-splitting of the data is included in the script. </span></p><p class="c5 c11"><span class="c2"></span></p><h2 class="c21" id="h.xzo7spd8fu7r"><span class="c29">Source</span></h2><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.xcc8bukh7iru"><span class="c27 c1">problem2.m</span></h3><a id="t.83fbb1b926149bf61fea9d57d2cab3a94cc20b94"></a><a id="t.3"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">%%% Problem 2: SVM</span><span class="c4"><br><br></span><span class="c12">%% load data from .txt files</span><span class="c4"><br>training_data = dlmread(</span><span class="c8">&#39;pima_train.txt&#39;</span><span class="c4">);<br>testing_data = dlmread(</span><span class="c8">&#39;pima_test.txt&#39;</span><span class="c4">);<br><br></span><span class="c12">%% shuffle data and adjust split</span><span class="c4"><br>data = [training_data; testing_data] </span><span class="c12">% combine data</span><span class="c4"><br>[r, c] = </span><span class="c9">size</span><span class="c4">(data) <br>shuf_data = data </span><span class="c12">% new matrix of same size</span><span class="c4"><br>idx=randperm(r) </span><span class="c12">% random permutation array</span><span class="c4"><br></span><span class="c17 c1">for</span><span class="c4">&nbsp;</span><span class="c9">i</span><span class="c4">=</span><span class="c3">1</span><span class="c4">:r <br> &nbsp; &nbsp;shuf_data(</span><span class="c9">i</span><span class="c4">,:)=data(idx(</span><span class="c9">i</span><span class="c4">),:); </span><span class="c12">% shuffle data</span><span class="c4"><br></span><span class="c17 c1">end</span><span class="c4"><br></span><span class="c12">% create new tt split</span><span class="c4"><br>training_data = shuf_data(</span><span class="c3">1</span><span class="c4">:</span><span class="c3">690</span><span class="c4">,:);<br>testing_data = shuf_data(</span><span class="c3">691</span><span class="c4">:r,:);<br><br></span><span class="c12">%% training</span><span class="c4"><br>X = training_data(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">8</span><span class="c4">);<br>Y_in = training_data(:,</span><span class="c3">9</span><span class="c4">);<br>cost = </span><span class="c3">1.0</span><span class="c4">;<br>[W,b] = svml(X,Y_in,cost);<br><br></span><span class="c12">%% get model performance on training data</span><span class="c4"><br></span><span class="c12">% get y and y_hat</span><span class="c4"><br>Y_test = training_data(:,</span><span class="c3">9</span><span class="c4">);<br>Y_hat = double(training_data(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">8</span><span class="c4">)*W+b &gt;</span><span class="c3">0</span><span class="c4">);<br></span><span class="c12">%training data size</span><span class="c4"><br>training_size = </span><span class="c9">size</span><span class="c4">(training_data)<br></span><span class="c12">% calculate accuracy</span><span class="c4"><br>train_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == Y_test)/</span><span class="c9">size</span><span class="c4">(Y_test,</span><span class="c3">1</span><span class="c4">)<br></span><span class="c12">% classification error</span><span class="c4"><br>train_error = </span><span class="c3">1</span><span class="c4">-train_accuracy<br></span><span class="c12">% build confusion matrix</span><span class="c4"><br>train_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~Y_test), Y_hat&#39;</span><span class="c4">*~Y_test;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*Y_test, Y_hat&#39;</span><span class="c4">*Y_test]<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br><br></span><span class="c12">%% get model performance on testing data</span><span class="c4"><br></span><span class="c12">% get y and y_hat</span><span class="c4"><br>Y_test = testing_data(:,</span><span class="c3">9</span><span class="c4">);<br>Y_hat = double(testing_data(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">8</span><span class="c4">)*W+b &gt;</span><span class="c3">0</span><span class="c4">);<br></span><span class="c12">% test data size</span><span class="c4"><br>testing_size=</span><span class="c9">size</span><span class="c4">(testing_data)<br></span><span class="c12">% calculate accuracy</span><span class="c4"><br>test_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == Y_test)/</span><span class="c9">size</span><span class="c4">(Y_test,</span><span class="c3">1</span><span class="c4">)<br><br></span><span class="c12">% classification error</span><span class="c4"><br>test_error = </span><span class="c3">1</span><span class="c4">-test_accuracy<br><br></span><span class="c12">% build confusion matrix</span><span class="c4"><br>test_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~Y_test), Y_hat&#39;</span><span class="c4">*~Y_test;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*Y_test, Y_hat&#39;</span><span class="c4">*Y_test]<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><h1 class="c28" id="h.p3mwi8cqwz4r"><span class="c0">Optional</span></h1><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span>This section was done with the use of the </span><span class="c1">LS-SVM Lab</span><span>&nbsp;open source software which can be can be accessed here: </span><span class="c36"><a class="c31" href="https://www.google.com/url?q=https://www.esat.kuleuven.be/sista/lssvmlab/&amp;sa=D&amp;ust=1558603928241000">https://www.esat.kuleuven.be/sista/lssvmlab/</a></span></p><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.wquj49i3cip"><span class="c19">Problem 1: Do I know how to work this?</span></h3><p class="c5"><span class="c2">Applying the library to the data from problem two did not seem to be a feasible starting point since the data was higher dimensional than I would be able to visualize, and therefore have a difficult time seeing if I was using the library properly. So I decided to start applying multiple SVM models to the first problem. </span></p><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.vjvkmqxj4r56"><span class="c19">Source</span></h3><a id="t.b438e9fe5f833b9c1f0ea43b4a01242fc56a0ca0"></a><a id="t.4"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">%% Problem 1 with SVM</span><span class="c4"><br><br></span><span class="c12">% enter the data manually</span><span class="c4"><br>class1 = [</span><span class="c3">-1</span><span class="c4">&nbsp;</span><span class="c3">-1</span><span class="c4">; </span><span class="c3">2</span><span class="c4">&nbsp;</span><span class="c3">0</span><span class="c4">; </span><span class="c3">2</span><span class="c4">&nbsp;</span><span class="c3">1</span><span class="c4">; </span><span class="c3">0</span><span class="c4">&nbsp;</span><span class="c3">1</span><span class="c4">; </span><span class="c3">0.5</span><span class="c4">&nbsp;</span><span class="c3">1.5</span><span class="c4">];<br>class2 = [</span><span class="c3">3.5</span><span class="c4">&nbsp;</span><span class="c3">2.5</span><span class="c4">; </span><span class="c3">3</span><span class="c4">&nbsp;</span><span class="c3">4</span><span class="c4">; </span><span class="c3">5</span><span class="c4">&nbsp;</span><span class="c3">2</span><span class="c4">; </span><span class="c3">5.5</span><span class="c4">&nbsp;</span><span class="c3">3</span><span class="c4">];<br><br>XY = [[class1, ones(size(class1,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">)];<br> &nbsp; &nbsp;[class2, -ones(size(class2,</span><span class="c3">1</span><span class="c4">),</span><span class="c3">1</span><span class="c4">)]];<br><br>X = XY(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">2</span><span class="c4">)<br>Y = XY(:,</span><span class="c3">3</span><span class="c4">)<br><br></span><span class="c12">%% auto tune linear model</span><span class="c4"><br>type = </span><span class="c8">&#39;classification&#39;</span><span class="c4">;<br>L_fold = </span><span class="c3">10</span><span class="c4">; </span><span class="c12">% L-fold crossvalidation</span><span class="c4"><br>model = initlssvm(X,Y,type,[],[],</span><span class="c8">&#39;lin_kernel&#39;</span><span class="c4">);<br>model = tunelssvm(model,</span><span class="c8">&#39;simplex&#39;</span><span class="c4">,</span><span class="c8">&#39;crossvalidatelssvm&#39;</span><span class="c4">,{L_fold,</span><span class="c8">&#39;misclass&#39;</span><span class="c4">});<br>model = trainlssvm(model);<br>plotlssvm(model);<br><br></span><span class="c12">%% auto tune polynomial model</span><span class="c4"><br>type = </span><span class="c8">&#39;classification&#39;</span><span class="c4">;<br>L_fold = </span><span class="c3">10</span><span class="c4">; </span><span class="c12">% L-fold crossvalidation</span><span class="c4"><br>model = initlssvm(X,Y,type,[],[],</span><span class="c8">&#39;poly_kernel&#39;</span><span class="c4">);<br>model = tunelssvm(model,</span><span class="c8">&#39;simplex&#39;</span><span class="c4">,</span><span class="c8">&#39;crossvalidatelssvm&#39;</span><span class="c4">,{L_fold,</span><span class="c8">&#39;misclass&#39;</span><span class="c4">});<br>model = trainlssvm(model);<br>plotlssvm(model);<br><br></span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.vhd44bfze86t"><span class="c19">Results</span></h3><h4 class="c23" id="h.k29bf7n6hhsg"><span>Linear Kernel</span></h4><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image2.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h4 class="c23" id="h.knl8stjc69hy"><span>Polynomial Kernel</span></h4><p class="c5"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 468.00px;"><img alt="" src="images/image6.jpg" style="width: 624.00px; height: 468.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">So we see that the library is functioning properly and handling the learning of both weight vectors as well as hyper parameters, like polynomial order, on its own by leveraging k-fold cross-validation. We can now move on to using the Problem 2.</span></p><p class="c5 c11"><span class="c2"></span></p><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.7g28kjphulr1"><span class="c19">Problem 2: No Visuals</span></h3><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span class="c2">In this section we will not be able to visually analyze how the model learns since the dimensionality of the data is too high. We will therefore rely only on the testing performance on the data, as we did in problem 2 of the main assignment.</span></p><p class="c5 c11"><span class="c2"></span></p><h4 class="c23" id="h.7kgzzm3nu7ir"><span class="c14">Load Data</span></h4><p class="c5 c11"><span class="c2"></span></p><a id="t.c7a63f8724b85beb56a8e1794a718fa47420156e"></a><a id="t.5"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">% Download package at: https://www.esat.kuleuven.be/sista/lssvmlab/</span><span class="c4"><br><br></span><span class="c12">% load data from .txt files</span><span class="c4"><br>training_data = dlmread(</span><span class="c8">&#39;pima_train.txt&#39;</span><span class="c4">);<br>testing_data = dlmread(</span><span class="c8">&#39;pima_test.txt&#39;</span><span class="c4">);<br><br></span><span class="c12">% training data</span><span class="c4"><br>X = training_data(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">8</span><span class="c4">);<br>Y = </span><span class="c3">2</span><span class="c4">*training_data(:,</span><span class="c3">9</span><span class="c4">) - </span><span class="c3">1</span><span class="c4">;<br><br></span><span class="c12">% testing data</span><span class="c4"><br>Xt = testing_data(:,</span><span class="c3">1</span><span class="c4">:</span><span class="c3">8</span><span class="c4">);<br>Yt = </span><span class="c3">2</span><span class="c4">*testing_data(:,</span><span class="c3">9</span><span class="c4">) - </span><span class="c3">1</span><span class="c4">;<br>Yt = (Yt+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">;</span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><h4 class="c23" id="h.s2mvk2950wr8"><span class="c14">Linear Kernel</span></h4><a id="t.0539a8d78e6c3da25e277d06a9eed551b6241ac7"></a><a id="t.6"></a><table class="c18"><tbody><tr class="c6"></tr></tbody></table><p class="c7 c11"><span class="c2"></span></p><a id="t.313dd45981f4b10eaed9cec477a78c5cc8c1d134"></a><a id="t.7"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">%% auto tune linear model</span><span class="c4"><br>type = </span><span class="c8">&#39;classification&#39;</span><span class="c4">;<br>L_fold = </span><span class="c3">10</span><span class="c4">; </span><span class="c12">% L-fold crossvalidation</span><span class="c4"><br>model = initlssvm(X,Y,type,[],[],</span><span class="c8">&#39;lin_kernel&#39;</span><span class="c4">);<br>model = tunelssvm(model,</span><span class="c8">&#39;simplex&#39;</span><span class="c4">,</span><span class="c8">&#39;crossvalidatelssvm&#39;</span><span class="c4">,{L_fold,</span><span class="c8">&#39;misclass&#39;</span><span class="c4">});<br>model = trainlssvm(model);<br><br></span><span class="c12">% calculate training performance</span><span class="c4"><br>Y_hat = simlssvm(model,X);<br>Y_hat = (Y_hat+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">;<br>train_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == ((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">)) /</span><span class="c9">size</span><span class="c4">(Y,</span><span class="c3">1</span><span class="c4">)<br>train_error = </span><span class="c3">1</span><span class="c4">-train_accuracy<br>train_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~((Y+1)/2)), Y_hat&#39;</span><span class="c4">*~((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">);<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*((Y+1)/2), Y_hat&#39;</span><span class="c4">*((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">)]<br><br></span><span class="c12">% calculate testing performance</span><span class="c4"><br>Y_hat = simlssvm(model,Xt);<br>Y_hat = (Y_hat+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">;<br>test_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == Yt)/</span><span class="c9">size</span><span class="c4">(Yt,</span><span class="c3">1</span><span class="c4">)<br>test_error = </span><span class="c3">1</span><span class="c4">-test_accuracy<br>test_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~Yt), Y_hat&#39;</span><span class="c4">*~Yt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*Yt, Y_hat&#39;</span><span class="c4">*Yt]<br><br></span></p></td></tr></tbody></table><h4 class="c23" id="h.ky7f7ll7r1bs"><span>Linear Results</span></h4><a id="t.b6724813e2e640dc8d368844676091160b20d578"></a><a id="t.8"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c4">train_accuracy = </span><span class="c3">0.7662</span><span class="c4"><br>train_error = </span><span class="c3">0.2338</span><span class="c30 c4"><br>train_confusion_matrix =<br></span></p><p class="c7"><span class="c4">&nbsp; &nbsp;</span><span class="c3">301</span><span class="c4">&nbsp; &nbsp; </span><span class="c3">38</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">88</span><span class="c4">&nbsp; &nbsp;</span><span class="c3">112</span><span class="c30 c4"><br></span></p><p class="c7"><span class="c4">test_accuracy = </span><span class="c3">0.7860</span><span class="c4"><br>test_error = </span><span class="c3">0.2140</span><span class="c4 c30"><br>test_confusion_matrix =<br></span></p><p class="c7"><span class="c4">&nbsp; &nbsp;</span><span class="c3">143</span><span class="c4">&nbsp; &nbsp; </span><span class="c3">18</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">31</span><span class="c4">&nbsp; &nbsp; </span><span class="c3">37</span></p></td></tr></tbody></table><h4 class="c23" id="h.74o6aduyg1d1"><span>Polynomial Kernel</span></h4><a id="t.313d82ed84be15a6b272fb477abc3b04c3120e0b"></a><a id="t.9"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c12">%% auto tune polinomial model</span><span class="c4"><br>type = </span><span class="c8">&#39;classification&#39;</span><span class="c4">;<br>L_fold = </span><span class="c3">10</span><span class="c4">; </span><span class="c12">% L-fold crossvalidation</span><span class="c4"><br>model = initlssvm(X,Y,type,[],[],</span><span class="c8">&#39;poly_kernel&#39;</span><span class="c4">);<br>model = tunelssvm(model,</span><span class="c8">&#39;simplex&#39;</span><span class="c4">,</span><span class="c8">&#39;crossvalidatelssvm&#39;</span><span class="c4">,{L_fold,</span><span class="c8">&#39;misclass&#39;</span><span class="c4">});<br>model = trainlssvm(model);<br><br></span><span class="c12">% calculate training performance</span><span class="c4"><br>Y_hat = simlssvm(model,X);<br>Y_hat = (Y_hat+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">;<br>train_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == ((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">)) /</span><span class="c9">size</span><span class="c4">(Y,</span><span class="c3">1</span><span class="c4">)<br>train_error = </span><span class="c3">1</span><span class="c4">-train_accuracy<br>train_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~((Y+1)/2)), Y_hat&#39;</span><span class="c4">*~((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">);<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*((Y+1)/2), Y_hat&#39;</span><span class="c4">*((Y+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">)]<br><br></span><span class="c12">% calculate testing performance</span><span class="c4"><br>Y_hat = simlssvm(model,Xt);<br>Y_hat = (Y_hat+</span><span class="c3">1</span><span class="c4">)/</span><span class="c3">2</span><span class="c4">;<br>test_accuracy = sum(Y_hat(:,</span><span class="c3">1</span><span class="c4">) == Yt)/</span><span class="c9">size</span><span class="c4">(Yt,</span><span class="c3">1</span><span class="c4">)<br>test_error = </span><span class="c3">1</span><span class="c4">-test_accuracy<br>test_confusion_matrix = [~Y_hat</span><span class="c8">&#39;*double(~Yt), Y_hat&#39;</span><span class="c4">*~Yt;<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;~Y_hat</span><span class="c8">&#39;*Yt, Y_hat&#39;</span><span class="c4">*Yt]</span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><h4 class="c23" id="h.o4vr1r12vfqc"><span class="c14">Polynomial Results</span></h4><a id="t.a488ecff417483e412655fc0d799549f9eabd583"></a><a id="t.10"></a><table class="c18"><tbody><tr class="c6"><td class="c15" colspan="1" rowspan="1"><p class="c7"><span class="c4">train_accuracy = </span><span class="c3">0.8571</span><span class="c4"><br>train_error = </span><span class="c3">0.1429</span><span class="c4"><br>train_confusion_matrix =<br><br> &nbsp; </span><span class="c3">308</span><span class="c4">&nbsp; &nbsp; </span><span class="c3">31</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">46</span><span class="c4">&nbsp; &nbsp;</span><span class="c3">154</span><span class="c4"><br><br>test_accuracy = </span><span class="c3">0.7205</span><span class="c4"><br>test_error = </span><span class="c3">0.2795</span><span class="c30 c4"><br>test_confusion_matrix =</span></p><p class="c7 c11"><span class="c30 c4"></span></p><p class="c7"><span class="c4">&nbsp; &nbsp;</span><span class="c3">126</span><span class="c4">&nbsp; &nbsp; </span><span class="c3">35</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">29</span><span class="c4">&nbsp; &nbsp; </span><span class="c30 c3">39</span></p><p class="c7 c11"><span class="c30 c3"></span></p><p class="c7"><span class="c30 c3">Obtained hyper-parameters: [gamma t degree]: 6.3019 0.35276 &nbsp;3</span></p></td></tr></tbody></table><p class="c5 c11"><span class="c2"></span></p><p class="c5 c11"><span class="c2"></span></p><h3 class="c13" id="h.5cy0p0l327td"><span class="c19">Conclusion</span></h3><p class="c5 c11"><span class="c2"></span></p><p class="c5"><span>As we can see, our linear kernel gets similar results to that of the implementation in problem 2 of the main assignment. We use then use an automatic procedure for tuning the hyperparameters of the polynomial model, which results in a degree three (cubic) kernel. We see with the linear model that again the training error is larger that the test error. When we switch to the polynomial model we are able to fit to the training data much more precisely, boosting our training accuracy by almost 10%. This however comes at a cost; the testing accuracy falls off relative to the testing score of the linear model. This is a clear sign of overfitting. It therefore seems that the linear model would be the more proper model to use because it will generalize better.</span></p></body></html>